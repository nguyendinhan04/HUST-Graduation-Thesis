{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16d32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03107733",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"data/sqlite/jobs.db\")\n",
    "cur = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22933201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables: [('job_links',), ('sqlite_sequence',)]\n"
     ]
    }
   ],
   "source": [
    "# show all tables name\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cur.fetchall()\n",
    "print(\"Tables:\", tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4e4ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "INSERT INTO job_links (url, normalized_url, status, attempts, last_error, next_try_at)\n",
    "VALUES \n",
    "('https://example.com/job/1', 'https://example.com/job/1', 'pending', 0, NULL, '2025-11-11 10:00:00'),\n",
    "('https://example.com/job/2', 'https://example.com/job/2', 'failed', 3, 'Timeout error', '2025-11-11 11:00:00'),\n",
    "('https://example.com/job/3', 'https://example.com/job/3', 'completed', 1, NULL, NULL),\n",
    "('https://example.com/job/4', 'https://example.com/job/4', 'pending', 0, NULL, '2025-11-12 09:00:00'),\n",
    "('https://example.com/job/5', 'https://example.com/job/5', 'failed', 2, 'Connection refused', '2025-11-12 10:30:00');\n",
    "\"\"\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b934e89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs: [(1, 'https://example.com/job/1', 'https://example.com/job/1', 'pending', 0, None, '2025-11-11 10:00:00', '2025-11-10 13:21:20', '2025-11-10 13:21:20'), (2, 'https://example.com/job/2', 'https://example.com/job/2', 'failed', 3, 'Timeout error', '2025-11-11 11:00:00', '2025-11-10 13:21:20', '2025-11-10 13:21:20'), (3, 'https://example.com/job/3', 'https://example.com/job/3', 'completed', 1, None, None, '2025-11-10 13:21:20', '2025-11-10 13:21:20'), (4, 'https://example.com/job/4', 'https://example.com/job/4', 'pending', 0, None, '2025-11-12 09:00:00', '2025-11-10 13:21:20', '2025-11-10 13:21:20'), (5, 'https://example.com/job/5', 'https://example.com/job/5', 'failed', 2, 'Connection refused', '2025-11-12 10:30:00', '2025-11-10 13:21:20', '2025-11-10 13:21:20')]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"select * from job_links\")\n",
    "jobs = cur.fetchall()\n",
    "print(\"Jobs:\", jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd270dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0                          1                          2          3  4  \\\n",
      "0  1  https://example.com/job/1  https://example.com/job/1    pending  0   \n",
      "1  2  https://example.com/job/2  https://example.com/job/2     failed  3   \n",
      "2  3  https://example.com/job/3  https://example.com/job/3  completed  1   \n",
      "3  4  https://example.com/job/4  https://example.com/job/4    pending  0   \n",
      "4  5  https://example.com/job/5  https://example.com/job/5     failed  2   \n",
      "\n",
      "                    5                    6                    7  \\\n",
      "0                None  2025-11-11 10:00:00  2025-11-10 13:21:20   \n",
      "1       Timeout error  2025-11-11 11:00:00  2025-11-10 13:21:20   \n",
      "2                None                 None  2025-11-10 13:21:20   \n",
      "3                None  2025-11-12 09:00:00  2025-11-10 13:21:20   \n",
      "4  Connection refused  2025-11-12 10:30:00  2025-11-10 13:21:20   \n",
      "\n",
      "                     8  \n",
      "0  2025-11-10 13:21:20  \n",
      "1  2025-11-10 13:21:20  \n",
      "2  2025-11-10 13:21:20  \n",
      "3  2025-11-10 13:21:20  \n",
      "4  2025-11-10 13:21:20  \n"
     ]
    }
   ],
   "source": [
    "jobs_df = pd.DataFrame(jobs)\n",
    "print(jobs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cebbdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import pendulum\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.utils.task_group import TaskGroup\n",
    "\n",
    "# đường dẫn tới file chứa keywords (1 dòng = 1 keyword)\n",
    "KEYWORDS_FILE = \"/path/to/keywords.txt\"  # <-- đổi path này\n",
    "\n",
    "# timezone (theo dev metadata của bạn)\n",
    "tz = pendulum.timezone(\"Asia/Bangkok\")\n",
    "\n",
    "# đọc keyword từ file txt\n",
    "def read_keywords(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        # loại bỏ dòng rỗng và trim\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "keywords = read_keywords(KEYWORDS_FILE)\n",
    "\n",
    "# hàm thực thi crawl cho 1 keyword (thay bằng hàm crawl thực của bạn)\n",
    "def crawl_keyword(keyword, **context):\n",
    "    # ví dụ: in ra, thực tế bạn gọi logic crawl ở đây\n",
    "    print(f\"Start crawling for keyword: {keyword}\")\n",
    "    # TODO: call your crawler logic (requests/bs4/scrapy)\n",
    "    # raise Exception(...) để test retry\n",
    "    return f\"done: {keyword}\"\n",
    "\n",
    "# cấu hình mặc định DAG\n",
    "default_args = {\n",
    "    \"owner\": \"you\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# base start date (ngày bắt đầu chung). Điều chỉnh nếu muốn bắt đầu từ ngày khác.\n",
    "# lưu ý: minute offset sẽ cộng vào giờ này\n",
    "base_start = tz.datetime(2025, 11, 11, 0, 0, 0)  # ví dụ bắt đầu từ 2025-11-11 00:00 Asia/Bangkok\n",
    "\n",
    "# tạo nhiều DAG động\n",
    "for idx, kw in enumerate(keywords):\n",
    "    # offset 15 phút cho mỗi keyword\n",
    "    offset_minutes = (idx * 15) % 60\n",
    "    offset_hours = (idx * 15) // 60\n",
    "\n",
    "    # start_date cho DAG này (shift bằng offset)\n",
    "    start_date = base_start.add(hours=offset_hours, minutes=offset_minutes)\n",
    "\n",
    "    dag_id = f\"crawl_keyword_{idx+1}_{kw.replace(' ', '_').replace('/', '_')}\".lower()\n",
    "\n",
    "    dag = DAG(\n",
    "        dag_id=dag_id,\n",
    "        default_args=default_args,\n",
    "        description=f\"Crawl for keyword: {kw}\",\n",
    "        schedule_interval=timedelta(days=1),  # chạy hàng ngày\n",
    "        start_date=start_date,\n",
    "        catchup=False,               # tránh chạy các dag runs cũ khi deploy lần đầu\n",
    "        max_active_runs=1,\n",
    "        tags=[\"crawler\", \"keywords\"],\n",
    "    )\n",
    "\n",
    "    # mỗi DAG có 1 task hoặc nhiều task tuỳ nhu cầu\n",
    "    with dag:\n",
    "        crawl = PythonOperator(\n",
    "            task_id=\"crawl_keyword\",\n",
    "            python_callable=crawl_keyword,\n",
    "            op_kwargs={\"keyword\": kw},\n",
    "            provide_context=True,\n",
    "        )\n",
    "\n",
    "    # bắt buộc: expose DAG vào globals để Airflow load được (pattern thường dùng)\n",
    "    globals()[dag_id] = dag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00eaafe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data-analyst-hr', '1933814.html']\n"
     ]
    }
   ],
   "source": [
    "link = \"https://www.topcv.vn/viec-lam/data-analyst-hr/1933814.html?ta_source=JobSearchList_LinkDetail&u_sr_id=ZHh0k8QsiUUy9gvUe1LljByND22jhDhvPAolZ07R_1762749550\"\n",
    "\n",
    "print(link.split('?')[0].split('/')[-2: ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90641b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE job_links_topcv( \n",
    "id integer primary key autoincrement,\n",
    "url text not null,\n",
    "hash_value text not null,\n",
    "status text not null default 'pending',\n",
    "attempts integer not null default 0, \n",
    "last_error text,\n",
    "next_try_at datetime,\n",
    "created_at datetime default current_timestamp,\n",
    "updated_at datetime default current_timestamp, source varchar(50),\n",
    "unique(hash_value)\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rent-price-predict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
